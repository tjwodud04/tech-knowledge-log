<!doctype html>
<html lang="ko">
<head>
  <meta charset="utf-8">
  <title>DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models">
  <link rel="stylesheet" href="/tech-knowledge-log/style.css?v=10">
  <style>
    /* Post-specific overrides only */
    .post-content h1 {
      font-size: 2rem;
      font-weight: 800;
      line-height: 1.3;
      margin-bottom: 2rem;
      color: #111827;
    }
    
    .topic-meta {
      font-size: 0.95rem;
      color: #6b7280;
      line-height: 1.8;
      margin-bottom: 2rem;
    }
    
    .topic-meta strong {
      font-weight: 700;
      color: #111827;
    }
    
    .topic-meta > div {
      margin: 0.5rem 0;
    }
    
    .topic-section {
      background: #f9fafb;
      border: 1px solid #e5e7eb;
      padding: 1.75rem;
      border-radius: 12px;
      margin: 2rem 0;
    }
    
    .topic-section h2 {
      margin: 0 0 1.25rem;
      font-size: 1.35rem;
      font-weight: 700;
      color: #1a1a1a;
    }
    
    .topic-section p {
      margin: 0.8rem 0;
      font-size: 1rem;
      color: #374151;
      line-height: 1.75;
    }
    
    .arxiv-links {
      display: flex;
      gap: 0.75rem;
      margin-top: 1.5rem;
      flex-wrap: wrap;
    }
    
    .arxiv-link,
    .hf-link {
      display: inline-block;
      padding: 0.65rem 1.35rem;
      border-radius: 6px;
      font-weight: 600;
      text-decoration: none;
      font-size: 0.95rem;
      line-height: 1;
      transition: all 0.2s ease;
      color: white !important;
    }
    
    .arxiv-link {
      background: #3b82f6;
    }
    
    .arxiv-link:hover {
      background: #2563eb;
      transform: translateY(-1px);
    }
    
    .hf-link {
      background: #ffb400;
    }
    
    .hf-link:hover {
      background: #e69a00;
      transform: translateY(-1px);
    }
    
    /* Responsive adjustments */
    @media screen and (max-width: 768px) {
      .post-content {
        padding: 1.5rem 1rem;
      }
      
      .post-content h1 {
        font-size: 1.5rem;
      }
      
      .topic-section {
        padding: 1.25rem;
      }
      
      .topic-section h2 {
        font-size: 1.15rem;
      }
    }
  </style>
</head>
<body>
  <header>
    <div class="header-content">
      <a href="/tech-knowledge-log/index.html" class="site-title">Tech Knowledge Log</a>
      <nav>
        <a href="/tech-knowledge-log/archive.html">Archive</a>
      </nav>
    </div>
  </header>
  <main class="container">
    <article class="post-content">
      <h1>DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models</h1>
      <div class="topic-meta">
        <div><strong>저자 / Authors:</strong> DeepSeek-AI, Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin</div>
        <div><strong>발행일 / Published:</strong> 2025-12-04</div>
        <div><strong>Arxiv ID:</strong> 2512.02556</div>
        <div><strong>Affiliation:</strong> DeepSeek</div>
      </div>

      <section class="topic-section">
        <h2>내용</h2>
        <p>DeepSeek-V3.2는 계산 효율성과 추론·에이전트 성능을 동시에 향상시킨 모델입니다. 핵심 기술로는 긴 문맥에서 성능을 유지하면서 계산 복잡도를 크게 낮춘 DeepSeek Sparse Attention(DSA), 대규모 후속 학습(포스트트레이닝)과 강화학습을 결합한 확장 가능한 학습 프레임워크, 그리고 도구 활용 상황에 맞는 대규모 에이전트 과제 합성 파이프라인이 제시됩니다. 고연산 버전인 DeepSeek-V3.2-Speciale는 GPT-5를 능가하고 Gemini-3.0-Pro 수준의 추론 능력을 보이며, 2025년 IMO·IOI에서 금메달급 성과를 달성했다고 보고합니다. 제안된 합성 파이프라인은 상호작용 환경에서의 일반화와 지시 이행 강건성을 크게 개선한다고 합니다.</p>
      </section>
      <section class="topic-section">
        <h2>Content</h2>
        <p>We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.</p>
        
        <div class="arxiv-links">
          <a href="https://arxiv.org/abs/2512.02556" class="arxiv-link" target="_blank" rel="noopener">View on Arxiv</a>
          <a href="https://huggingface.co/papers/2512.02556" class="hf-link" target="_blank" rel="noopener">View on HF</a>
        </div>
      </section>
    </article>
  </main>
  <footer>
    <p>&copy; 2025 Tech Knowledge Log. All rights reserved.</p>
  </footer>
  <script src="/tech-knowledge-log/script.js?v=10"></script>
</body>
</html>