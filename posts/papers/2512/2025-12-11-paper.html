<!doctype html>
<html lang="ko">
<head>
  <meta charset="utf-8">
  <title>Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance">
  <link rel="stylesheet" href="/tech-knowledge-log/style.css?v=10">
  <style>
    /* Post-specific overrides only */
    .post-content h1 {
      font-size: 2rem;
      font-weight: 800;
      line-height: 1.3;
      margin-bottom: 2rem;
      color: #111827;
    }
    
    .topic-meta {
      font-size: 0.95rem;
      color: #6b7280;
      line-height: 1.8;
      margin-bottom: 2rem;
    }
    
    .topic-meta strong {
      font-weight: 700;
      color: #111827;
    }
    
    .topic-meta > div {
      margin: 0.5rem 0;
    }
    
    .topic-section {
      background: #f9fafb;
      border: 1px solid #e5e7eb;
      padding: 1.75rem;
      border-radius: 12px;
      margin: 2rem 0;
    }
    
    .topic-section h2 {
      margin: 0 0 1.25rem;
      font-size: 1.35rem;
      font-weight: 700;
      color: #1a1a1a;
    }
    
    .topic-section p {
      margin: 0.8rem 0;
      font-size: 1rem;
      color: #374151;
      line-height: 1.75;
    }
    
    .arxiv-links {
      display: flex;
      gap: 0.75rem;
      margin-top: 1.5rem;
      flex-wrap: wrap;
    }
    
    .arxiv-link,
    .hf-link {
      display: inline-block;
      padding: 0.65rem 1.35rem;
      border-radius: 6px;
      font-weight: 600;
      text-decoration: none;
      font-size: 0.95rem;
      line-height: 1;
      transition: all 0.2s ease;
      color: white !important;
    }
    
    .arxiv-link {
      background: #3b82f6;
    }
    
    .arxiv-link:hover {
      background: #2563eb;
      transform: translateY(-1px);
    }
    
    .hf-link {
      background: #ffb400;
    }
    
    .hf-link:hover {
      background: #e69a00;
      transform: translateY(-1px);
    }
    
    /* Responsive adjustments */
    @media screen and (max-width: 768px) {
      .post-content {
        padding: 1.5rem 1rem;
      }
      
      .post-content h1 {
        font-size: 1.5rem;
      }
      
      .topic-section {
        padding: 1.25rem;
      }
      
      .topic-section h2 {
        font-size: 1.15rem;
      }
    }
  </style>
</head>
<body>
  <header>
    <div class="header-content">
      <a href="/tech-knowledge-log/index.html" class="site-title">Tech Knowledge Log</a>
      <nav>
        <a href="/tech-knowledge-log/archive.html">Archive</a>
      </nav>
    </div>
  </header>
  <main class="container">
    <article class="post-content">
      <h1>Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance</h1>
      <div class="topic-meta">
        <div><strong>저자 / Authors:</strong> Ruihang Chu, Yefei He, Zhekai Chen, Shiwei Zhang, Xiaogang Xu, Bin Xia, Dingdong Wang, Hongwei Yi, Xihui Liu, Hengshuang Zhao</div>
        <div><strong>발행일 / Published:</strong> 2025-12-11</div>
        <div><strong>Arxiv ID:</strong> 2512.08765</div>
        <div><strong>Affiliation:</strong> TongyiLab</div>
      </div>

      <section class="topic-section">
        <h2>내용</h2>
        <p>Wan-Move는 동작 제어를 비디오 생성 모델에 간단하고 확장 가능하게 도입한 프레임워크입니다. 물체 움직임을 조밀한 포인트 궤적으로 표현하고 이를 잠재 공간으로 투영한 뒤 첫 프레임의 특징을 각 궤적을 따라 전파해 정렬된 시공간 특징 맵을 만들어 원래 조건 피처를 모션 인지형으로 업데이트합니다. 이 업데이트된 조건은 아키텍처 변경 없이 기존의 이미지→비디오 모델(예: Wan-I2V-14B)에 그대로 통합되어 보조 모션 인코더 없이 세밀한 모션 제어와 확장성 있는 파인튜닝을 가능하게 합니다. 대규모 학습과 사용자 평가에서 5초 480p 비디오의 모션 제어 성능이 상용 Motion Brush와 유사하게 우수했으며, 평가를 위한 대규모 고품질 벤치마크 MoveBench와 코드·모델·데이터를 공개했습니다.</p>
      </section>
      <section class="topic-section">
        <h2>Content</h2>
        <p>We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.</p>
        
        <div class="arxiv-links">
          <a href="https://arxiv.org/abs/2512.08765" class="arxiv-link" target="_blank" rel="noopener">View on Arxiv</a>
          <a href="https://huggingface.co/papers/2512.08765" class="hf-link" target="_blank" rel="noopener">View on HF</a>
        </div>
      </section>
    </article>
  </main>
  <footer>
    <p>&copy; 2025 Tech Knowledge Log. All rights reserved.</p>
  </footer>
  <script src="/tech-knowledge-log/script.js?v=10"></script>
</body>
</html>