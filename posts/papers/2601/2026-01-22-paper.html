<!doctype html>
<html lang="ko">
<head>
  <meta charset="utf-8">
  <title>Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization">
  <link rel="stylesheet" href="/tech-knowledge-log/style.css?v=10">
  <style>
    /* Post-specific overrides only */
    .post-content h1 {
      font-size: 2rem;
      font-weight: 800;
      line-height: 1.3;
      margin-bottom: 2rem;
      color: #111827;
    }
    
    .topic-meta {
      font-size: 0.95rem;
      color: #6b7280;
      line-height: 1.8;
      margin-bottom: 2rem;
    }
    
    .topic-meta strong {
      font-weight: 700;
      color: #111827;
    }
    
    .topic-meta > div {
      margin: 0.5rem 0;
    }
    
    .topic-section {
      background: #f9fafb;
      border: 1px solid #e5e7eb;
      padding: 1.75rem;
      border-radius: 12px;
      margin: 2rem 0;
    }
    
    .topic-section h2 {
      margin: 0 0 1.25rem;
      font-size: 1.35rem;
      font-weight: 700;
      color: #1a1a1a;
    }
    
    .topic-section p {
      margin: 0.8rem 0;
      font-size: 1rem;
      color: #374151;
      line-height: 1.75;
    }
    
    .arxiv-links {
      display: flex;
      gap: 0.75rem;
      margin-top: 1.5rem;
      flex-wrap: wrap;
    }
    
    .arxiv-link,
    .hf-link {
      display: inline-block;
      padding: 0.65rem 1.35rem;
      border-radius: 6px;
      font-weight: 600;
      text-decoration: none;
      font-size: 0.95rem;
      line-height: 1;
      transition: all 0.2s ease;
      color: white !important;
    }
    
    .arxiv-link {
      background: #3b82f6;
    }
    
    .arxiv-link:hover {
      background: #2563eb;
      transform: translateY(-1px);
    }
    
    .hf-link {
      background: #ffb400;
    }
    
    .hf-link:hover {
      background: #e69a00;
      transform: translateY(-1px);
    }
    
    /* Responsive adjustments */
    @media screen and (max-width: 768px) {
      .post-content {
        padding: 1.5rem 1rem;
      }
      
      .post-content h1 {
        font-size: 1.5rem;
      }
      
      .topic-section {
        padding: 1.25rem;
      }
      
      .topic-section h2 {
        font-size: 1.15rem;
      }
    }
  </style>
</head>
<body>
  <header>
    <div class="header-content">
      <a href="/tech-knowledge-log/index.html" class="site-title">Tech Knowledge Log</a>
      <nav>
        <a href="/tech-knowledge-log/archive.html">Archive</a>
      </nav>
    </div>
  </header>
  <main class="container">
    <article class="post-content">
      <h1>Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization</h1>
      <div class="topic-meta">
        <div><strong>저자 / Authors:</strong> Hao Luo, Ye Wang, Wanpeng Zhang, Sipeng Zheng, Ziheng Xi, Chaoyi Xu, Haiweng Xu, Haoqi Yuan, Chi Zhang, Yiqing Wang</div>
        <div><strong>발행일 / Published:</strong> 2026-01-22</div>
        <div><strong>Arxiv ID:</strong> 2601.12993</div>
        <div><strong>Affiliation:</strong> BeingBeyond</div>
      </div>

      <section class="topic-section">
        <h2>내용</h2>
        <p>Being-H0.5는 다양한 로봇 플랫폼 간의 교차-체현(embodiment) 일반화를 목표로 한 기초 Vision–Language–Action 모델입니다. 저자들은 인간 상호작용 궤적을 보편적 '모국어'로 삼는 인간 중심 학습을 제안하고, 30개 로봇에 걸쳐 35,000시간 이상의 멀티모달 데이터를 모은 UniHand-2.0을 제공하여 데이터 부족 문제를 완화합니다. 이 모델은 이질적 로봇 제어를 의미적으로 정렬된 슬롯으로 매핑하는 Unified Action Space와, 공통 모터 프리미티브와 체현별 전문가를 분리하는 Mixture-of-Flow 기반 Mixture-of-Transformers 아키텍처를 도입합니다. 또한 Manifold-Preserving Gating과 Universal Async Chunking으로 감각 변화와 제어 지연에 대한 견고성을 확보해 LIBERO(98.9%)와 RoboCasa(53.9%) 등 시뮬레이션 벤치마크와 다섯 로봇 플랫폼에서 우수한 교차-체현 성능을 보였습니다.</p>
      </section>
      <section class="topic-section">
        <h2>Content</h2>
        <p>We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal "mother tongue" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.</p>
        
        <div class="arxiv-links">
          <a href="https://arxiv.org/abs/2601.12993" class="arxiv-link" target="_blank" rel="noopener">View on Arxiv</a>
          <a href="https://huggingface.co/papers/2601.12993" class="hf-link" target="_blank" rel="noopener">View on HF</a>
        </div>
      </section>
    </article>
  </main>
  <footer>
    <p>&copy; 2025 Tech Knowledge Log. All rights reserved.</p>
  </footer>
  <script src="/tech-knowledge-log/script.js?v=10"></script>
</body>
</html>