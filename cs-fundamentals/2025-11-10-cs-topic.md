# Gradient Descent

## 내용 / Content
경사 하강법(Gradient Descent)은 기계 학습에서 손실 함수를 최소화하기 위해 사용되는 최적화 알고리즘입니다. 이 알고리즘은 함수의 기울기를 사용해 최저점을 찾기 위해 반복적으로 파라미터를 업데이트합니다. 경사 하강법의 기본 아이디어는 현재 위치에서 기울기가 가장 낮은 방향으로 이동하는 것입니다. 여러 변형이 있으며, 스토캐스틱 경사 하강법(Stochastic Gradient Descent)이나 미니배치 경사 하강법(Mini-batch Gradient Descent)도 자주 사용됩니다.

Gradient Descent is an optimization algorithm used in machine learning to minimize a loss function. It operates by iteratively updating parameters in the direction of the steepest descent, identified by the gradient of the function. The core idea is to move in the direction that reduces the loss at the current position. Various variants exist, including Stochastic Gradient Descent (SGD) and Mini-batch Gradient Descent, which are widely employed for efficiency in training.

## 활용 / Applications
경사 하강법은 이미지 인식, 자연어 처리, 추천 시스템 등 다양한 기계 학습 및 딥러닝 모델에서 사용됩니다. It is utilized across various machine learning and deep learning models, including image recognition, natural language processing, and recommendation systems.