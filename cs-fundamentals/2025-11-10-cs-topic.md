# Gradient Descent

## 내용 / Content
경사 하강법(Gradient Descent)은 머신러닝 모델의 손실 함수를 최소화하는데 사용되는 최적화 알고리즘입니다. 이 방법은 현재 위치에서 기울기(gradient)를 계산하고, 이를 바탕으로 손실 함수의 값을 줄이기 위해 파라미터를 업데이트합니다. 간단히 말해, 경사 하강법은 모델의 성능을 개선하기 위해 파라미터 공간에서 최저점을 찾아가는 과정입니다.

경사 하강법은 여러 변형이 있으며, 그 중 가장 기본적인 버전은 배치 경사 하강법(Batch Gradient Descent)과 확률적 경사 하강법(Stochastic Gradient Descent, SGD)이 있습니다. 배치 경사 하강법은 전체 데이터셋을 사용하여 기울기를 계산하지만, SGD는 하나의 샘플만 사용하여 빠르게 업데이트를 진행합니다. 이러한 특징 덕분에 경사 하강법은 많은 머신러닝 알고리즘에서 널리 사용됩니다.

## 활용 / Applications
경사 하강법은 회귀분석, 신경망 훈련 등 다양한 머신러닝 애플리케이션에서 사용됩니다. 현대의 딥러닝 모델에서도 필수적인 최적화 알고리즘으로 자리 잡고 있습니다.

Gradient Descent is widely used in machine learning applications such as regression analysis and training neural networks. It has become an essential optimization algorithm in modern deep learning models.